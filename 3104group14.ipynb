{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c28529329c8d4d4e8f3391d45b19c6b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "",
              "5D3X6.mp4",
              "5O0YS.mp4",
              "5EFQ7.mp4",
              "2E8GU.mp4",
              "raheem_dance.mp4",
              "3LMTS.mp4",
              "8ASRQ.mp4",
              "3JJ7C.mp4",
              "0RJKT.mp4",
              "52CKM.mp4",
              "3IPI3.mp4",
              "4GLAP.mp4"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Select a file:",
            "description_tooltip": null,
            "disabled": false,
            "index": 5,
            "layout": "IPY_MODEL_a1106cc68f7443ecadd6d96f6035c5ea",
            "style": "IPY_MODEL_6ecab009f8844b0baafcd033303a5b27"
          }
        },
        "a1106cc68f7443ecadd6d96f6035c5ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ecab009f8844b0baafcd033303a5b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone -b pose-extraction https://github.com/YUyou0/ict3104-team14-2023.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuuKAySGjX-Z",
        "outputId": "5b34f0da-c280-4a74-af8e-e34a87f4f410"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ict3104-team14-2023'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 85 (delta 10), reused 21 (delta 4), pack-reused 56\u001b[K\n",
            "Receiving objects: 100% (85/85), 57.13 MiB | 27.66 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VU-qTv-vZ-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d7e349-a78b-475e-b89e-47a87ff765fd"
      },
      "source": [
        "%cd /content/ict3104-team14-2023/\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "BODY_PARTS = { \"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
        "               \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
        "               \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
        "               \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18 }\n",
        "\n",
        "POSE_PAIRS = [ [\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
        "               [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
        "               [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"],\n",
        "               [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"],\n",
        "               [\"REye\", \"REar\"], [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"] ]\n",
        "\n",
        "width = 368\n",
        "height = 368\n",
        "inWidth = width\n",
        "inHeight = height\n",
        "\n",
        "net = cv.dnn.readNetFromTensorflow(\"graph_opt.pb\")\n",
        "thr = 0.2\n",
        "\n",
        "def poseDetector(frame):\n",
        "    frameWidth = frame.shape[1]\n",
        "    frameHeight = frame.shape[0]\n",
        "\n",
        "    net.setInput(cv.dnn.blobFromImage(frame, 1.0, (inWidth, inHeight), (127.5, 127.5, 127.5), swapRB=True, crop=False))\n",
        "    out = net.forward()\n",
        "    out = out[:, :19, :, :]  # MobileNet output [1, 57, -1, -1], we only need the first 19 elements\n",
        "\n",
        "    assert(len(BODY_PARTS) == out.shape[1])\n",
        "\n",
        "    points = []\n",
        "    for i in range(len(BODY_PARTS)):\n",
        "        # Slice heatmap of corresponging body's part.\n",
        "        heatMap = out[0, i, :, :]\n",
        "\n",
        "        _, conf, _, point = cv.minMaxLoc(heatMap)\n",
        "        x = (frameWidth * point[0]) / out.shape[3]\n",
        "        y = (frameHeight * point[1]) / out.shape[2]\n",
        "        points.append((int(x), int(y)) if conf > thr else None)\n",
        "\n",
        "    for pair in POSE_PAIRS:\n",
        "        partFrom = pair[0]\n",
        "        partTo = pair[1]\n",
        "        assert(partFrom in BODY_PARTS)\n",
        "        assert(partTo in BODY_PARTS)\n",
        "\n",
        "        idFrom = BODY_PARTS[partFrom]\n",
        "        idTo = BODY_PARTS[partTo]\n",
        "\n",
        "        if points[idFrom] and points[idTo]:\n",
        "            cv.line(frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
        "            cv.ellipse(frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
        "            cv.ellipse(frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
        "\n",
        "    t, _ = net.getPerfProfile()\n",
        "\n",
        "    return frame"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ict3104-team14-2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select a video for pose extraction"
      ],
      "metadata": {
        "id": "UtPCSboGkquP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from IPython.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Specify the folder path where you want to list files\n",
        "folder_path = '/content/ict3104-team14-2023/input_videos/charades'  # Change this to your desired folder path\n",
        "\n",
        "# Get a list of files in the folder\n",
        "file_list = os.listdir(folder_path)\n",
        "file_list.insert(0,\"\")  # Insert \"\" at the start of the list\n",
        "\n",
        "# Create a dropdown widget with file options\n",
        "file_dropdown = widgets.Dropdown(\n",
        "    options=file_list,\n",
        "    description='Select a file:',\n",
        ")\n",
        "\n",
        "# Display the dropdown widget\n",
        "display(file_dropdown)\n",
        "\n",
        "# Function to handle file selection\n",
        "def select_file(change):\n",
        "    global selected_file\n",
        "    selected_file = change.new\n",
        "    print(f\"Selected file: {selected_file}\")\n",
        "    # Store the selected file name in a variable or use it as needed\n",
        "    # For example, you can assign it to a variable like this:\n",
        "    # selected_file_name = selected_file\n",
        "\n",
        "# Attach the select_file function to the dropdown's observe method\n",
        "file_dropdown.observe(select_file, names='value')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "c28529329c8d4d4e8f3391d45b19c6b9",
            "a1106cc68f7443ecadd6d96f6035c5ea",
            "6ecab009f8844b0baafcd033303a5b27"
          ]
        },
        "id": "J9N1W9_lksok",
        "outputId": "949b8cc0-2776-4dae-c34e-0fd678455a42"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Select a file:', options=('', '5D3X6.mp4', '5O0YS.mp4', '5EFQ7.mp4', '2E8GU.mp4', 'raheeâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c28529329c8d4d4e8f3391d45b19c6b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected file: raheem_dance.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(selected_file)\n",
        "selected_file_trimmed = selected_file[:-4]\n",
        "print(selected_file_trimmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZYsY7amnJfv",
        "outputId": "8378171c-6956-4786-89d4-44385836de00"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raheem_dance.mp4\n",
            "raheem_dance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the pose extraction process"
      ],
      "metadata": {
        "id": "IhBRo1m0lNAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ict3104-team14-2023/input_videos/charades/\n",
        "!pwd\n",
        "\n",
        "import cv2\n",
        "cap = cv2.VideoCapture(selected_file)\n",
        "ret, frame = cap.read()\n",
        "frame_height, frame_width, _ = frame.shape\n",
        "out = cv2.VideoWriter(f'{selected_file_trimmed}_pose.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
        "print(\"Processing Video...\")\n",
        "while cap.isOpened():\n",
        "  ret, frame = cap.read()\n",
        "  if not ret:\n",
        "    out.release()\n",
        "    break\n",
        "  output = poseDetector(frame)\n",
        "  out.write(output)\n",
        "out.release()\n",
        "print(\"Done processing video\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-j365-ylPi9",
        "outputId": "4e90169b-75de-4ee9-cdc0-15ecf3c6b740"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ict3104-team14-2023/input_videos/charades\n",
            "/content/ict3104-team14-2023/input_videos/charades\n",
            "Processing Video...\n",
            "Done processing video\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove the video background leaving only the pose extraction"
      ],
      "metadata": {
        "id": "FwI8wBgSs1-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ict3104-team14-2023/input_videos/charades/\n",
        "!pwd\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the video\n",
        "cap = cv2.VideoCapture('raheem_dance_pose.avi')  # Replace 'input_video.mp4' with your video file\n",
        "output_video_path = 'raheem_dance_pose_extracted.mov'\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, (frame_width, frame_height), isColor=True)\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert the frame to HSV color space\n",
        "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Define the lower and upper bounds for bright green and red in HSV color space\n",
        "    lower_green = np.array([35, 100, 250])\n",
        "    upper_green = np.array([85, 255, 255])\n",
        "\n",
        "    lower_red1 = np.array([0, 100, 250])\n",
        "    upper_red1 = np.array([80, 255, 255])\n",
        "\n",
        "    # Create masks to isolate bright green and red regions\n",
        "    mask_green = cv2.inRange(hsv, lower_green, upper_green)\n",
        "    mask_red1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
        "\n",
        "    # Combine the green and red masks\n",
        "    mask_combined = cv2.bitwise_or(mask_green, mask_red1)\n",
        "\n",
        "    # Create an all-black frame\n",
        "    black_background = np.zeros_like(frame)\n",
        "\n",
        "    # Set the pixels in the black background frame to the original frame where the mask is white\n",
        "    black_background[mask_combined > 0] = frame[mask_combined > 0]\n",
        "\n",
        "    # Write the frame to the output video\n",
        "    out.write(black_background)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "print(\"Pose extraction processing done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLsxFceJs7De",
        "outputId": "5dcf743a-787a-4bd3-b2c9-14d93cacfab2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ict3104-team14-2023/input_videos/charades\n",
            "/content/ict3104-team14-2023/input_videos/charades\n",
            "Pose extraction processing done\n"
          ]
        }
      ]
    }
  ]
}